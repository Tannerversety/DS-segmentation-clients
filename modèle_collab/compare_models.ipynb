{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import scipy as sc\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import manifold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn import decomposition\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import (silhouette_score, adjusted_rand_score)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison des prédictions (modèle manuel et collaboratif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ouvrir les données \n",
    "\n",
    "df = pd.read_csv('fc_data.csv')\n",
    "df.loc[:,'InvoiceDate'] = pd.to_datetime(df.InvoiceDate,errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ce notebook a pour objectif de comparer la pertinence des vecteurs caractéristiques utilisateurs dans chaqu'un des deux modèles (manuel et collaboratif).\n",
    "On suppose que plus les prédictions engendrées par le modèle sont bonnes, plus les vecteurs caractéristiques qui le paramétrisent sont représentatifs de la réalité. \n",
    "Pour rappel, dans le cas collaboratif, les vecteurs caractéristiques émergent de l'algorithme, il est donc \"censé\" capturer les phénoménes sous-jacents d'une manière plus efficace que la simple intuition. Cependant, l'algorithme, dans la forme que nous avons implémenté, ne peut que détecter des phénoménes corrélés avec la nature des items consomés par l'ensemble des utilisateurs. Si les items sont semblables ou que les utilisateurs ne font pas de différence à l'achat entre les différents items, les vecteurs caractéristiques qui émergeront seront moins pertinents.\n",
    "Dans le cas manuel, nous nous basons sur notre intuition pour définir des vecteurs caractéristiques. Nous avons, entre autre, définie des caractéristiques de type transactionelles ou date pas forcément corrélées avec la nature des items.\n",
    "Il est donc légitime de se demander lequel de ces deux modèles est le plus efficace et si ils sont complémentaires. On compare leurs prédictions évaluées via la métrique \"rank\" définie dans le notebook (\"implicit_cf\").\n",
    "L'algorithme de prédiction du modèle collaboratif est explicité dans le notebook (\"implicit_cf).\n",
    "En ce qui concerne l'algorithme de prédiction du modèle manuel, nous implémentons une prédiction basée sur la distance euclidienne séparant l'item prédit du barycentre des prédictions. \n",
    "Notons que nous considérons un sous espace de 500 utilisateurs et de 500 items (les plus populaires). Les résultats obtenus peuvent donc être biaisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mélanger les données\n",
    "\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on rajoute une lettre N (pour number) avant chaque numéro de produit\n",
    "df.loc[:,'InvoiceNo'] = 'N' + df['InvoiceNo'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalt(u_i): \n",
    "    \"\"\"define a new evaluation metric based on the rank of the recommandation mark\"\"\"\n",
    "    \n",
    "    #u_i = shuffle(u_i)\n",
    "    rank_list = []\n",
    "    \n",
    "    for i in range (u_i.shape[0]):\n",
    "        \n",
    "        if (u_i.iloc[i] == 1):\n",
    "            \n",
    "            rank_list.append(i)  \n",
    "    \n",
    "    penalt = 0\n",
    "    \n",
    "    for i in range (len(rank_list)):\n",
    "        \n",
    "        penalt = penalt + rank_list[i] - np.count_nonzero(u_i[0:rank_list[i]])\n",
    "        \n",
    "    penalt = penalt / len(rank_list)\n",
    "    \n",
    "    # get penalt in %\n",
    "    penalt = (penalt * 100) / (u_i.shape[0] - len(rank_list))\n",
    "        \n",
    "    return(penalt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_counts(col_name,df):       \n",
    "    \n",
    "    values_count = pd.DataFrame(df[col_name].dropna().value_counts())\n",
    "    #print (values_count.shape)\n",
    "    values_count.columns = ['count']\n",
    "    # convert the index column into a regular column.\n",
    "    values_count[col_name] = [ str(i) for i in values_count.index ]\n",
    "    # add a column with the percentage of each data point to the sum of all data points.\n",
    "    values_count['percent'] = values_count['count'].div(values_count['count'].sum()).multiply(100).round(2)\n",
    "    # change the order of the columns.\n",
    "    values_count = values_count.reindex([col_name,'count','percent'],axis=1)\n",
    "    values_count.reset_index(drop=True,inplace=True)\n",
    "    return (values_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous séparons tout d'abord le jeu initial en un jeu de test et d'entrainement. Nous choisissons un certain nombre de commandes dans la base de donnée initiale et telle sorte à ce que le nombre de commandes dans le testset représente 30% du nombre de commandes totales. Nous construisons ensuite les jeux de données manuels avec caractéristiques et les jeux de données d'entrée (matrice des transactions items > utilisateurs) pour le modèle collaboratif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netoyage_items(mod_train,mod_test):\n",
    "    \n",
    "    # utilisateurs dans le jeux d'entrainement    \n",
    "    nb_users_train = plot_value_counts('CustomerID',df=mod_train)\n",
    "    print (nb_users_train.shape)\n",
    "    \n",
    "    # utilisateurs dans le jeux de test \n",
    "    nb_users_test = plot_value_counts('CustomerID',df=mod_test)\n",
    "    print (nb_users_test.shape)\n",
    "        \n",
    "    # trouver les utilisateurs qui ne sont pas communs \n",
    "    mod_test = mod_test[(mod_test.loc[:,'CustomerID'].isin(mod_train.loc[:,'CustomerID']))].reset_index(drop=True)\n",
    "    mod_train = mod_train[(mod_train.loc[:,'CustomerID'].isin(mod_test.loc[:,'CustomerID']))].reset_index(drop=True)\n",
    "    print (mod_test.shape)\n",
    "    print (mod_train.shape)\n",
    "        \n",
    "    return (mod_train, mod_test)\n",
    "\n",
    "def netoyage_users(mod_train,mod_test):\n",
    "    \n",
    "    # utilisateurs dans le jeux d'entrainement    \n",
    "    nb_users_train = plot_value_counts('StockCode',df=mod_train)\n",
    "    print (nb_users_train.shape)\n",
    "    \n",
    "    # utilisateurs dans le jeux de test \n",
    "    nb_users_test = plot_value_counts('StockCode',df=mod_test)\n",
    "    print (nb_users_test.shape)\n",
    "        \n",
    "    # trouver les utilisateurs qui ne sont pas communs \n",
    "    mod_test = mod_test[(mod_test.loc[:,'StockCode'].isin(mod_train.loc[:,'StockCode']))].reset_index(drop=True)\n",
    "    mod_train = mod_train[(mod_train.loc[:,'StockCode'].isin(mod_test.loc[:,'StockCode']))].reset_index(drop=True)\n",
    "    print (mod_test.shape)\n",
    "    print (mod_train.shape)\n",
    "        \n",
    "    return (mod_train, mod_test)\n",
    "\n",
    "\n",
    "\n",
    "def traintest_split(df):\n",
    "    \n",
    "    valeurs_cust_trans = shuffle(plot_value_counts('StockCode',df=df))\n",
    "    \n",
    "    \n",
    "    # effectuer une copie sur laquelle on va venir travailler dans le but d'accélérer la rapidité du programme\n",
    "    \n",
    "    df_test = df.copy()\n",
    "    df_train = df.copy()\n",
    "    \n",
    "            \n",
    "    for i in range (valeurs_cust_trans.shape[0]):\n",
    "        print(i)\n",
    "        \n",
    "        df_trans = df[df.loc[:,'StockCode'] == valeurs_cust_trans.iloc[i,0]]\n",
    "        \n",
    "        df_trans2 = plot_value_counts('InvoiceNo',df=df_trans)        \n",
    "        \n",
    "        # on retire 30% du jeux de données\n",
    "        for j in range (int(np.around(0.3*df_trans2.shape[0])+1)):\n",
    "            # on itére sur le nombre de premières transactions que l'on considère           \n",
    "\n",
    "            # on repére la liste des transactions correspondant à la première commande\n",
    "            df_trans_date = df_trans[df_trans.loc[:,'InvoiceDate'] == np.amin(df_trans.loc[:,'InvoiceDate'])]\n",
    "\n",
    "            ind_t = df.columns.get_loc('InvoiceNo')\n",
    "\n",
    "            # on identifie le numéro de cette commande\n",
    "            transaction = df_trans_date.iloc[0,ind_t]  \n",
    "\n",
    "            # on supprimme du trainset les transactions liées à cette commande           \n",
    "            df_train = df_train[df_train.loc[:,'InvoiceNo'] != transaction] \n",
    "\n",
    "            # on supprimme de \"df_trans\" les transactions liées à cette commande\n",
    "            df_trans = df_trans[df_trans.loc[:,'InvoiceNo'] != transaction]      \n",
    "        \n",
    "        \n",
    "\n",
    "    # on crée le testset \n",
    "    \n",
    "    df_test = df[(df.loc[:,'InvoiceNo'].isin(df_train.loc[:,'InvoiceNo'])==False)]        \n",
    "    \n",
    "           \n",
    "    return (df_train , df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fonctions necessaires au fonctionnement de l'algorithme collaboratif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let us define some important functions for our collaboratif algorithm\n",
    "\n",
    "\n",
    "def nonzeros(m, row):\n",
    "    for index in range(m.indptr[row], m.indptr[row+1]):\n",
    "        yield m.indices[index], m.data[index]\n",
    "        \n",
    "        \n",
    "\n",
    "def alternating_least_squares_cg(Cui, factors, regularization=0.01, iterations=15):\n",
    "    users, items = Cui.shape\n",
    "\n",
    "    # initialize factors randomly\n",
    "    X = np.random.rand(users, factors) * 0.01\n",
    "    Y = np.random.rand(items, factors) * 0.01\n",
    "\n",
    "    Cui, Ciu = Cui.tocsr(), Cui.T.tocsr()\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        least_squares_cg(Cui, X, Y, regularization)\n",
    "        least_squares_cg(Ciu, Y, X, regularization)\n",
    "\n",
    "    return (X, Y)\n",
    "\n",
    "\n",
    "def least_squares_cg(Cui, X, Y, regularization, cg_steps=3):\n",
    "    users, factors = X.shape\n",
    "    YtY = Y.T.dot(Y) + regularization * np.eye(factors)\n",
    "\n",
    "    for u in range(users):\n",
    "        # start from previous iteration\n",
    "        x = X[u]\n",
    "\n",
    "        # calculate residual r = (YtCuPu - (YtCuY.dot(Xu), without computing YtCuY\n",
    "        r = -YtY.dot(x)\n",
    "        for i, confidence in nonzeros(Cui, u):\n",
    "            r += (confidence - (confidence - 1) * Y[i].dot(x)) * Y[i]\n",
    "\n",
    "        p = r.copy()\n",
    "        rsold = r.dot(r)\n",
    "\n",
    "        for it in range(cg_steps):\n",
    "            # calculate Ap = YtCuYp - without actually calculating YtCuY\n",
    "            Ap = YtY.dot(p)\n",
    "            for i, confidence in nonzeros(Cui, u):\n",
    "                Ap += (confidence - 1) * Y[i].dot(p) * Y[i]\n",
    "\n",
    "            # standard CG update\n",
    "            alpha = rsold / p.dot(Ap)\n",
    "            x += alpha * p\n",
    "            r -= alpha * Ap\n",
    "            rsnew = r.dot(r)\n",
    "            p = r + (rsnew / rsold) * p\n",
    "            rsold = rsnew\n",
    "\n",
    "        X[u] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(user_id, sparse_user_item, user_vecs, item_vecs, num_items=10):\n",
    "    \"\"\"fonction de recommandation \"\"\"\n",
    "\n",
    "    user_interactions = sparse_user_item[user_id,:].toarray()\n",
    "\n",
    "    user_interactions = user_interactions.reshape(-1) + 1\n",
    "    user_interactions[user_interactions > 1] = 0\n",
    "\n",
    "    rec_vector = user_vecs[user_id,:].dot(item_vecs.T).toarray()\n",
    "\n",
    "    min_max = MinMaxScaler()\n",
    "    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "    recommend_vector = user_interactions * rec_vector_scaled\n",
    "\n",
    "    item_idx = np.argsort(recommend_vector)[::-1][:num_items]\n",
    "\n",
    "    artists = []\n",
    "    scores = []\n",
    "\n",
    "    for idx in item_idx:\n",
    "        artists.append(data.artist.loc[data.artist_id == idx].iloc[0])\n",
    "        scores.append(recommend_vector[idx])\n",
    "\n",
    "    recommendations = pd.DataFrame({'artist': artists, 'score': scores})\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prelim_cf(df):\n",
    "    \"\"\" établir la matrice de transaction utilisateurs > items\"\"\"\n",
    "    \n",
    "        \n",
    "    df_cf = df[['InvoiceNo','CustomerID','Quantity','StockCode','UnitPrice']]\n",
    "    \n",
    "    ind_cust = df_cf.columns.get_loc('CustomerID')\n",
    "    ind_q = df_cf.columns.get_loc('Quantity')\n",
    "    ind_price = df_cf.columns.get_loc('UnitPrice')\n",
    "    ind_stock = df_cf.columns.get_loc('StockCode')\n",
    "    ind_numb = df_cf.columns.get_loc('InvoiceNo')\n",
    "\n",
    "    valeurs_cust = plot_value_counts('CustomerID',df=df_cf)\n",
    "    valeurs_stock = plot_value_counts('StockCode',df=df_cf)     \n",
    "\n",
    "    # associer à chaque couple produit/consommateur un Rui\n",
    "\n",
    "    # supprimer les colonnes inutiles \n",
    "    valeurs_cust_rui = valeurs_cust_t.drop(columns = ['count','percent']) \n",
    "    \n",
    "\n",
    "\n",
    "    # supprimer les colonnes inutiles \n",
    "    valeurs_cust_rui = valeurs_cust_t.drop(columns = ['count','percent'])\n",
    "\n",
    "    # créer un vecteur numpy \n",
    "    vect = np.zeros(valeurs_cust_t.shape[0])\n",
    "    for i in range (valeurs_stock_t.shape[0]):\n",
    "        valeurs_cust_rui['{}'.format(valeurs_stock_t.iloc[i,0])] = vect[:]\n",
    "\n",
    "    valeurs_cust_pui = valeurs_cust_rui.copy()\n",
    "\n",
    "    # on itére sur les produits \n",
    "    for i in range (valeurs_stock_t.shape[0]):\n",
    "        \n",
    "        df_cf1 = df_cf[(df_cf.iloc[:,ind_stock] == valeurs_stock_t.iloc[i,0])]        \n",
    "        # on itére sur les clients\n",
    "        for j in range (valeurs_cust_rui.shape[0]):\n",
    "            df_ij = df_cf1[(df_cf1.iloc[:,ind_cust] == valeurs_cust_rui.iloc[j,0])]\n",
    "            \n",
    "            valeurs_cust_rui.iloc[j,i+1] = df_ij.shape[0] \n",
    "             \n",
    "    # associer aux utilisateurs une valeur numérique \n",
    "    vect = np.zeros(valeurs_cust_rui.shape[0])\n",
    "    for i in range (valeurs_cust_rui.shape[0]):\n",
    "        vect[i] = i\n",
    "    # sauvegarder un df_pandas pour retrouver le numéro correspondant\n",
    "    d = {'number': valeurs_cust_rui.iloc[:,0], 'value': vect[:]}\n",
    "    df_users = pd.DataFrame(data=d)\n",
    "\n",
    "    # associer aux items une valeur numérique \n",
    "    vect = np.zeros(valeurs_stock_t.shape[0])\n",
    "    for i in range (valeurs_stock_t.shape[0]):\n",
    "        vect[i] = i\n",
    "    # sauvegarder un df_pandas pour retrouver le numéro correspondant\n",
    "    d = {'number': valeurs_stock_t.iloc[:,0], 'value': vect[:]}\n",
    "    df_items = pd.DataFrame(data=d)\n",
    "\n",
    "    # créer la matrice utilisateur \n",
    "    d = {'it0': df_users.iloc[:,1]}\n",
    "    for i in range (valeurs_stock_t.shape[0]-1):\n",
    "\n",
    "        d.update({'it{}'.format(i+1): df_users.iloc[:,1]})\n",
    "        users_matrix = pd.DataFrame(data=d)      \n",
    "\n",
    "    # créer la matrice item\n",
    "    d = {'it0': df_items.iloc[:,1]}\n",
    "    for i in range (valeurs_cust_rui.shape[0]-1):\n",
    "        d.update({'it{}'.format(i+1): df_items.iloc[:,1]})\n",
    "        items_matrix = pd.DataFrame(data=d)       \n",
    "\n",
    "        \n",
    "    # vecteur note\n",
    "    rating = np.matrix(valeurs_cust_rui.iloc[:,1::])\n",
    "    rating = rating.flatten().transpose()\n",
    "    \n",
    "    # vecteur user\n",
    "    um = np.matrix(users_matrix.iloc[:,:])\n",
    "    um = um.flatten().transpose()\n",
    "    \n",
    "    # vecteur item\n",
    "    it = np.matrix(items_matrix.iloc[:,:])\n",
    "    it = it.transpose().flatten().transpose()\n",
    "    \n",
    "    vect = np.zeros(it.shape)\n",
    "\n",
    "    d = {'try': vect[:,0]}\n",
    "    fc_matrix = pd.DataFrame(data=d)\n",
    "    fc_matrix['itemID'] = it[:,0]\n",
    "    fc_matrix['userID'] = um[:,0]\n",
    "    fc_matrix['rating'] = rating[:,0]\n",
    "    \n",
    "    \n",
    "    return (fc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fonctions necessaires au fonctionnement de l'algorithme manuel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(df):\n",
    "    \"\"\" définir les caractéristiques items \"\"\"\n",
    "    \n",
    "    ind_date = df.columns.get_loc('InvoiceDate')\n",
    "    df['DAY'] = df.iloc[:,ind_date].dt.weekday    \n",
    "    df['MONTH'] = df.iloc[:,ind_date].dt.month    \n",
    "    df['HOUR'] = df.iloc[:,ind_date].dt.hour\n",
    "    \n",
    "    \n",
    "    # repérer les indices\n",
    "    ind_h = df.columns.get_loc('HOUR')\n",
    "    ind_m = df.columns.get_loc('MONTH')\n",
    "    ind_m = df.columns.get_loc('DAY')\n",
    "    ind_price = df.columns.get_loc('UnitPrice')\n",
    "    ind_cust = df.columns.get_loc('StockCode')\n",
    "    ind_q = df.columns.get_loc('Quantity')\n",
    "    \n",
    "    ind_cbis = df.columns.get_loc('Country_bis')\n",
    "\n",
    "    valeurs_cust = plot_value_counts('StockCode',df=df)    \n",
    "      \n",
    "    \n",
    "    valeurs_month = plot_value_counts('MONTH',df=df)\n",
    "    valeurs_month.iloc[:,0] = pd.to_numeric(valeurs_month.iloc[:,0], errors='coerce').fillna(0, downcast='infer')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ---------------------------------------- Associer les caractéristiques moyennes ------------------------------------\n",
    "\n",
    "    #définir un vecteur numpy\n",
    "    vect = np.zeros(valeurs_cust.shape[0])\n",
    "    dataset_clust = valeurs_cust.iloc[:,[0]]\n",
    "\n",
    "    #ajouter les nouvelles caractéristiques\n",
    "    #dataset_clust['lasttransaction'] = vect[:]\n",
    "    dataset_clust['nb_unit'] = vect[:]\n",
    "    dataset_clust['prix_unit'] = vect[:]\n",
    "    dataset_clust['montant'] = vect[:]\n",
    "    dataset_clust['pays'] = vect[:]\n",
    "    dataset_clust['nb_user_diff'] = vect[:]\n",
    "    dataset_clust['nb_user_id'] = vect[:]       \n",
    "    dataset_clust['frequence'] = vect[:]\n",
    "    \n",
    "    \n",
    "    # tenir compte du jour de la semaine et de l'heure des transactions par 4 nouvelles variables\n",
    "    # le jour(heure) comptant le plus grand nombre de transaction (0 à 7) et la probabilité de transaction \n",
    "    # ce jour(heure) ie le ratio nombre de transaction enregistrées le jour du maximum de transaction sur le nombre total\n",
    "    # de transactions\n",
    "    dataset_clust['jourmax'] = vect[:]\n",
    "    dataset_clust['heuremax'] = vect[:]\n",
    "    dataset_clust['jourprob'] = vect[:]\n",
    "    dataset_clust['heureprob'] = vect[:]\n",
    "\n",
    "    # associer la valeur 1 lorsque le pays est uk et 0 sinon\n",
    "    df['Country_bis'][df.loc[:,'Country_bis'] == 'UK'] = 1\n",
    "    df['Country_bis'][df.loc[:,'Country_bis'] == 'Foreign'] = 0\n",
    "\n",
    "    # remplir les colonnes \n",
    "    for i in range (dataset_clust.shape[0]):\n",
    "        \n",
    "        df_trans =  df[df.iloc[:,ind_cust] == valeurs_cust.iloc[i,0]] \n",
    "        \n",
    "        dataset_clust.loc[i,'nb_unit'] = np.mean(df_trans.iloc[:,ind_q])\n",
    "        \n",
    "        dataset_clust.loc[i,'prix_unit'] = np.mean(df_trans.iloc[:,ind_price])\n",
    "        \n",
    "        dataset_clust.loc[i,'montant'] = np.mean(df_trans.iloc[:,ind_price].multiply(df_trans.iloc[:,ind_q]))\n",
    "        \n",
    "        dataset_clust.loc[i,'pays'] = df_trans.iloc[0,ind_cbis]\n",
    "        \n",
    "        dataset_clust.loc[i,'frequence'] = (np.amax(df_trans.loc[:,'InvoiceDate']) - \n",
    "                                            np.amin(df_trans.loc[:,'InvoiceDate']))/df_trans.shape[0]\n",
    "        \n",
    "        # convertir le temps en seconde      \n",
    "        dataset_clust.loc[i,'frequence'] = dataset_clust.loc[i,'frequence'].total_seconds()\n",
    "        dataset_clust.loc[i,'frequence'] = float(dataset_clust.loc[i,'frequence'])\n",
    "        \n",
    "        \n",
    "              \n",
    "        \n",
    "        valeurs_day = plot_value_counts('DAY',df=df)\n",
    "        valeurs_day.iloc[:,0] = pd.to_numeric(valeurs_day.iloc[:,0], errors='coerce').fillna(0, downcast='infer') \n",
    "        \n",
    "        dataset_clust['jourmax'] = valeurs_day.iloc[0,0]\n",
    "        dataset_clust['jourprob'] = valeurs_day.iloc[0,1]/np.sum(valeurs_day.iloc[0,:])\n",
    "        \n",
    "        valeurs_hour = plot_value_counts('HOUR',df=df)\n",
    "        valeurs_hour.iloc[:,0] = pd.to_numeric(valeurs_hour.iloc[:,0], errors='coerce').fillna(0, downcast='infer') \n",
    "        \n",
    "        dataset_clust['heuremax'] = valeurs_hour.iloc[0,0]    \n",
    "        dataset_clust['heureprob'] = valeurs_hour.iloc[0,1]/np.sum(valeurs_hour.iloc[0,:])\n",
    "        \n",
    "        valeurs_invoice = plot_value_counts('InvoiceNo',df=df_trans)\n",
    "        dataset_clust.loc[i,'nb_user_diff'] = np.mean(valeurs_invoice.iloc[:,1])\n",
    "        \n",
    "        valeurs_identiques = plot_value_counts('CustomerID',df=df_trans)\n",
    "        dataset_clust.loc[i,'nb_user_id'] = np.mean(valeurs_identiques.iloc[:,1]) \n",
    "        \n",
    "    return(dataset_clust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algo_manu(df_train,df_test,iu_train):  \n",
    "    \"\"\"algorithme de prédiction manuel \"\"\"\n",
    "     \n",
    "    # initialiser le score \n",
    "    score = 0 \n",
    "\n",
    "\n",
    "    ind_item = df_train.columns.get_loc('StockCode')   \n",
    "    \n",
    "    valeurs_user = plot_value_counts('CustomerID',df=df_train)\n",
    "    \n",
    "    divid = valeurs_user.shape[0]\n",
    "    print (valeurs_user.shape[0])\n",
    "\n",
    "    for i in range (valeurs_user.shape[0]):\n",
    "        print (i)\n",
    "        \n",
    "        # on regarde les transactions de chaque utilisateur dans le trainset\n",
    "        df_trans = df_train[df_train.loc[:,'CustomerID'] == valeurs_user.iloc[i,0]]\n",
    "       \n",
    "        # on regarde les transactions de chaque utilisateur dans le testset\n",
    "        df_trans_t = df_test[df_test.loc[:,'CustomerID'] == valeurs_user.iloc[i,0]]\n",
    "        \n",
    "\n",
    "        # visualiser l'occurence de chaque idem (poid du barycentre)\n",
    "        valeurs_stock = plot_value_counts('StockCode',df=df_trans)\n",
    "        \n",
    "        #valeurs_stock.iloc[:,1][valeurs_stock.iloc[:,1] > 0] = 1\n",
    "        \n",
    "      \n",
    "        \n",
    "        # idem pour le testset\n",
    "        valeurs_stock_t = plot_value_counts('StockCode',df=df_trans_t)\n",
    "        \n",
    "\n",
    "        #  Créer la matrice item/carac adaptée à l'utilisateur i\n",
    "        Mat_ic_red = iu_train[(iu_train.loc[:,'StockCode'].isin(valeurs_stock.iloc[:,0])==True)]\n",
    "        \n",
    "\n",
    "        # Trier les items\n",
    "        Mat_ic_red = Mat_ic_red.sort_values(by=['StockCode'])\n",
    "        valeurs_stock = valeurs_stock.sort_values(by=['StockCode'])\n",
    "\n",
    "        # calculer le barycentre pour l'utilisateur i\n",
    "\n",
    "        up = np.matrix(valeurs_stock.iloc[:,1])\n",
    "        \n",
    "        print (valeurs_stock)\n",
    "        print (Mat_ic_red)\n",
    "        ic = np.matrix(Mat_ic_red.iloc[:,1::])\n",
    "        scale = StandardScaler().fit(ic)\n",
    "        ic =  scale.transform(ic) \n",
    "        \n",
    "       \n",
    "        bar = (1. / np.sum(up)) *  np.dot(up,ic) \n",
    "        print (bar)\n",
    "             \n",
    "        # calcul de la distance euclidienne entre le barycentre et chaqu'un des points \n",
    "\n",
    "        # calculer uniquement pour les éléments non déjà commandés\n",
    "\n",
    "        Mat_ic_none = iu_train[(iu_train.loc[:,'StockCode'].isin(valeurs_stock.iloc[:,0])==False)]\n",
    "\n",
    "        vect = np.zeros(Mat_ic_none.shape[0])\n",
    "        Mat_ic_none['distance'] = vect[:]\n",
    "\n",
    "        ind_d = Mat_ic_none.columns.get_loc('distance')\n",
    "\n",
    "        X = np.matrix(Mat_ic_none.iloc[:,1:-1])\n",
    "        X =  scale.transform(X)      \n",
    "        \n",
    "       \n",
    "        for j in range (Mat_ic_none.shape[0]):        \n",
    "\n",
    "            Mat_ic_none.iloc[j,ind_d]= np.sqrt(np.dot((bar[0,:] - X[j,:]),np.transpose(bar[0,:] - X[j,:]))[0,0])\n",
    "            \n",
    "        # considérer dans le testset uniquement les éléments non communs au trainset\n",
    "        \n",
    "        valeurs_stock_t = valeurs_stock_t[(valeurs_stock_t.iloc[:,0].isin(valeurs_stock.iloc[:,0])==False)]\n",
    "        \n",
    "        # considérer les cas sans nouvelles transactions \n",
    "        if (valeurs_stock_t.shape[0] == 0):\n",
    "            \n",
    "            divid = divid - 1\n",
    "            \n",
    "        # calculer le rank            \n",
    "        if (valeurs_stock_t.shape[0] > 0):\n",
    "            \n",
    "            # associer la valeurs de l'occurence de l'item lorsque la transaction à lieu dans le testset et 0 sinon\n",
    "            vect = np.zeros(Mat_ic_none.shape[0])\n",
    "            Mat_ic_none['test'] = vect[:]\n",
    "            \n",
    "            # associer les valeurs des occurences lorsque la transaction à lieu\n",
    "            \n",
    "            for j in range (valeurs_stock_t.shape[0]):                \n",
    "                \n",
    "                #print (Mat_ic_none)         \n",
    "                Mat_ic_none.loc[:,'test'][Mat_ic_none.loc[:,'StockCode'] == valeurs_stock_t.iloc[j,0]] = valeurs_stock_t.iloc[j,1]\n",
    "                #print (Mat_ic_none)\n",
    "               \n",
    "            \n",
    "            # trier df_trans_t et Mat_ic_none par utilisateur\n",
    "            \n",
    "            Mat_ic_none = Mat_ic_none.sort_values(by=['StockCode'])\n",
    "            valeurs_stock_t = valeurs_stock_t.sort_values(by=['StockCode'])\n",
    "            \n",
    "                        \n",
    "            # trier Mat_ic_none par prédiction (de la distance la moins élevée à la plus élevée)\n",
    "            \n",
    "            Mat_ic_none = Mat_ic_none.sort_values(by='distance', ascending=True)\n",
    "           \n",
    "            # associer à la liste 'test' la valeurs 1 si l'occurence est superieure à 0\n",
    "            \n",
    "            Mat_ic_none.loc[:,'test'][Mat_ic_none.loc[:,'test'] > 0] = 1\n",
    "            \n",
    "                        \n",
    "            # calculate penalties\n",
    "            ind_test = Mat_ic_none.columns.get_loc('test')\n",
    "            penalties = penalt(u_i=Mat_ic_none.iloc[:,ind_test])\n",
    "            \n",
    "            score = score + penalties\n",
    "            \n",
    "            \n",
    "            \n",
    "    score = score / divid       \n",
    "\n",
    "\n",
    "    \n",
    "    return (score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on ne garde que les 100 items et utilisateurs principaux\n",
    "\n",
    "valeurs_cust = plot_value_counts('CustomerID',df=df)\n",
    "print (valeurs_cust.iloc[0:10,:])\n",
    "valeurs_cust = valeurs_cust.drop(valeurs_cust.index[0])\n",
    "\n",
    "valeurs_stock = plot_value_counts('StockCode',df=df)\n",
    "\n",
    "n_u = 500\n",
    "n_p = 500\n",
    "\n",
    "valeurs_stock_t = valeurs_stock.iloc[0:n_p,:]\n",
    "valeurs_cust_t = valeurs_cust.iloc[0:n_u,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df.shape)\n",
    "# ne garder que les utilisateurs communs : \n",
    "df = df[(df.loc[:,'CustomerID'].isin(valeurs_cust_t.iloc[:,0]))].reset_index(drop=True)\n",
    "\n",
    "# ne garder que les items communs\n",
    "df = df[(df.loc[:,'StockCode'].isin(valeurs_stock_t.iloc[:,0]))].reset_index(drop=True)\n",
    "print (df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on crée 1 jeux de test/entrainement \n",
    "\n",
    "train_set, test_set = traintest_split(df=df) \n",
    "\n",
    "# on s'assure que les items dans le trainset et testset sont identiques \n",
    "\n",
    "df_train, df_test = netoyage_items(mod_train=train_set,mod_test=test_set)[:]\n",
    "       \n",
    "# on s'assure que les users dans le trainset et testset sont identiques \n",
    "\n",
    "df_train, df_test = netoyage_users(mod_train=df_train,mod_test=df_test)[:]\n",
    "       \n",
    "\n",
    "#-------------------------MANUAL ----------------------------------------------\n",
    "\n",
    "# déterminer les caractéristiques items pour le trainset   \n",
    "\n",
    "train_citem = features(df_train)\n",
    "       \n",
    "\n",
    "# déterminer le score de prédiction \n",
    "\n",
    "score_manu = algo_manu(df_train=df_train,df_test=df_test,iu_train=train_citem)\n",
    "       \n",
    "\n",
    "#-------------------------COLABORATIF  ----------------------------------------------\n",
    "\n",
    "\n",
    "# travail préliminaire pour le testset\n",
    "\n",
    "test_prelim = prelim_cf(df_test)\n",
    "\n",
    "# travail préliminaire pour le trainset  \n",
    "\n",
    "train_prelim = prelim_cf(df_train)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ traite les transactions identiques entre le trainset et le testset  -------------------\n",
    "# associe la valeur -1 pour faciliter un traitement ulterieur\n",
    "\n",
    "\n",
    "# number of users \n",
    "users_testset = plot_value_counts(col_name='userID',df=test_prelim)\n",
    "users_testset.iloc[:,0] = pd.to_numeric(users_testset.iloc[:,0], errors='coerce').fillna(0, downcast='infer')\n",
    "\n",
    "\n",
    "# number of items \n",
    "items_testset = plot_value_counts(col_name='itemID',df=test_prelim)\n",
    "items_testset.iloc[:,0] = pd.to_numeric(items_testset.iloc[:,0], errors='coerce').fillna(0, downcast='infer')\n",
    "\n",
    "nb_users = users_testset.shape[0]\n",
    "nb_items = items_testset.shape[0]\n",
    "\n",
    "for ind_user in range (nb_users):\n",
    "    #print (ind_user)\n",
    "    \n",
    "    item_matrix = test_prelim[test_prelim.loc[:,'userID'] == users_testset.iloc[ind_user,0]]\n",
    "    item_matrix_tr = train_prelim[train_prelim.loc[:,'userID'] == users_testset.iloc[ind_user,0]] \n",
    "    \n",
    "    item_matrix = item_matrix.sort_values(by='itemID', ascending=False)\n",
    "    item_matrix_tr =item_matrix_tr.sort_values(by='itemID', ascending=False)\n",
    "    \n",
    "    \n",
    "    for i in range (item_matrix_tr.shape[0]):  \n",
    "        \n",
    "        if (item_matrix.iloc[i,3]>0):\n",
    "            \n",
    "            if (item_matrix_tr.iloc[i,3]>0):\n",
    "                \n",
    "                item_matrix.iloc[i,3] = -1\n",
    "                        \n",
    "        \n",
    "        \n",
    "    if (ind_user==0):\n",
    "        test_prelim2 = item_matrix\n",
    "        \n",
    "    if (ind_user!=0):\n",
    "    \n",
    "        test_prelim2  = pd.concat([test_prelim2, item_matrix],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul le modèle collaboratif pour les meilleurs hyperparamètres (voir implicit cf)\n",
    "\n",
    "data = train_prelim[['userID', 'itemID', 'rating']]\n",
    "\n",
    "#data = data_cf[['userID', 'itemID', 'rating']]\n",
    "\n",
    "data = data.rename(columns={'userID':'user','itemID':'artist','rating':'plays'})\n",
    "df = data.copy()\n",
    "print (data.columns)\n",
    "\n",
    "# Drop NaN columns\n",
    "data = data.dropna()\n",
    "data = data.copy()\n",
    "\n",
    "# Create a numeric user_id and artist_id column\n",
    "data['user'] = data['user'].astype(\"category\")\n",
    "data['artist'] = data['artist'].astype(\"category\")\n",
    "data['user_id'] = data['user'].cat.codes\n",
    "data['artist_id'] = data['artist'].cat.codes\n",
    "\n",
    "# The implicit library expects data as a item-user matrix so we\n",
    "# create two matricies, one for fitting the model (item-user) \n",
    "# and one for recommendations (user-item)\n",
    "sparse_item_user = sparse.csr_matrix((data['plays'].astype(float), (data['artist_id'], data['user_id'])))\n",
    "sparse_user_item = sparse.csr_matrix((data['plays'].astype(float), (data['user_id'], data['artist_id'])))\n",
    "\n",
    "# Calculate the confidence by multiplying it by our alpha value.\n",
    "alpha_val = 60\n",
    "data_conf = (sparse_item_user * alpha_val).astype('double')\n",
    "\n",
    "# Initialize the als model and fit it using the sparse item-user matrix\n",
    "reg_param = 0.1\n",
    "X, Y = alternating_least_squares_cg(Cui=data_conf ,factors=10, regularization=reg_param, iterations=10)\n",
    "\n",
    "# Get the user and item vectors from our trained model\n",
    "user_vecs = sparse.csr_matrix(X)\n",
    "item_vecs = sparse.csr_matrix(Y)\n",
    "\n",
    "\n",
    "# number of users \n",
    "users_testset = plot_value_counts(col_name='userID',df=train_prelim)\n",
    "users_testset.iloc[:,0] = pd.to_numeric(users_testset.iloc[:,0], errors='coerce').fillna(0, downcast='infer')\n",
    "\n",
    "# number of items \n",
    "items_testset = plot_value_counts(col_name='itemID',df=train_prelim)\n",
    "items_testset.iloc[:,0] = pd.to_numeric(items_testset.iloc[:,0], errors='coerce').fillna(0, downcast='infer')\n",
    "                \n",
    "\n",
    "nb_users = users_testset.shape[0]\n",
    "nb_items = items_testset.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------ COMPUTING NAIF CLASSIFIORS BASED ON TRAIN SET ----------------------------------\n",
    "\n",
    "\n",
    "# number of users \n",
    "users_testset = plot_value_counts(col_name='userID',df=train_prelim)\n",
    "users_testset.iloc[:,0] = pd.to_numeric(users_testset.iloc[:,0], errors='coerce').fillna(0, downcast='infer')\n",
    "\n",
    "# number of items \n",
    "items_testset = plot_value_counts(col_name='itemID',df=train_prelim)\n",
    "items_testset.iloc[:,0] = pd.to_numeric(items_testset.iloc[:,0], errors='coerce').fillna(0, downcast='infer')\n",
    "\n",
    "nb_users = users_testset.shape[0]\n",
    "nb_items = items_testset.shape[0]\n",
    "        \n",
    "                \n",
    "              \n",
    "# classifior based on the transaction number for each item\n",
    "items_naif1 = np.zeros(items_testset.shape[0])\n",
    "        \n",
    "# classifior based on the number of user who did a transaction for each item\n",
    "items_naif2 = np.zeros(items_testset.shape[0])\n",
    "        \n",
    "for i in range (items_testset.shape[0]):\n",
    "    \n",
    "            \n",
    "    df_trans = train_prelim[train_prelim.loc[:,'itemID'] == items_testset.iloc[i,0]]\n",
    "    items_naif1[i] = np.sum(df_trans.loc[:,'rating'])          \n",
    "            \n",
    "            \n",
    "    for j in range (users_testset.shape[0]): \n",
    "        \n",
    "                \n",
    "        df_trans2 = df_trans[df_trans.loc[:,'userID'] == users_testset.iloc[j,0]]\n",
    "        \n",
    "                    \n",
    "        if (df_trans2.iloc[0,3] > 0):                 \n",
    "                    \n",
    "            items_naif2[i] = items_naif2[i] + 1            \n",
    "                    \n",
    "        \n",
    "                        \n",
    "# arranger dans le dataframe pandas 'valeurs_items'\n",
    "items_testset['naif1'] = items_naif1[:]\n",
    "items_testset['naif2'] = items_naif2[:]\n",
    "        \n",
    "        \n",
    "# order items_testset by items and save in numpy arrays\n",
    "items_testset = items_testset.sort_values(by='itemID', ascending=False)\n",
    "npusers_items = np.matrix(items_testset.iloc[:,:])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Evaluer la performance --------------\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "        \n",
    "# init scores \n",
    "score = 0 \n",
    "score_nf1 = 0\n",
    "score_nf2 = 0\n",
    "\n",
    "\n",
    "nb_users = users_testset.shape[0]\n",
    "nb_items = items_testset.shape[0]\n",
    "        \n",
    "# number of transactions\n",
    "divid = nb_users        \n",
    "      \n",
    "# ------------------------------------ EVALUATING PERFORMENCE  --------------------------------------\n",
    "\n",
    "# Create recommendations for all users in the testset\n",
    "\n",
    "for ind_user in range (nb_users):\n",
    "\n",
    "    user_id = users_testset.iloc[ind_user,0]\n",
    "    \n",
    "                  \n",
    "    recommendations = recommend(user_id, sparse_user_item, user_vecs, item_vecs, num_items=nb_items) \n",
    "    \n",
    "\n",
    "    # order recommendations by items        \n",
    "    recommendations = recommendations.sort_values(by='artist', ascending=False)   \n",
    "            \n",
    "    # find the \"true\" rating array of items for user \"ind_user\"         \n",
    "    item_matrix = test_prelim2[test_prelim2.loc[:,'userID'] == users_testset.iloc[ind_user,0]]\n",
    "                    \n",
    "    sum_mat = np.sum(item_matrix.loc[:,'rating'])   \n",
    "     \n",
    "           \n",
    "    # cases with transactions\n",
    "    if (sum_mat > 0):     \n",
    "             \n",
    "        # order item_matrix by items                \n",
    "        item_matrix = item_matrix.sort_values(by='itemID', ascending=False)\n",
    "        npusers_matrix = np.matrix(item_matrix.iloc[:,0::])\n",
    "\n",
    "        \n",
    "        # add the \"true\" rating array of items for user \"ind_user\" \n",
    "        recommendations['rui'] = npusers_matrix[:,3]\n",
    "\n",
    "        # add the naif classif 1 and 2 \n",
    "        recommendations['naif1'] = npusers_items[:,3]\n",
    "        recommendations['naif2'] = npusers_items[:,4]\n",
    "       \n",
    "        recommendations = recommendations[recommendations.loc[:,'rui']>(-1)]\n",
    "        \n",
    "        recommendations.loc[:,'rui'][recommendations.loc[:,'rui'] > 0] = 1\n",
    "\n",
    "        # --------------------- Perform on predicted score --------------            \n",
    "        # order item by predicted score\n",
    "        \n",
    "        recommendations = recommendations.sort_values(by='score', ascending=False)\n",
    "\n",
    "        # calculate penalties \n",
    "        penalties = penalt(u_i=recommendations.iloc[:,2])\n",
    "        score = score + penalties\n",
    "\n",
    "\n",
    "        # --------------------- Perform on naif 1 --------------            \n",
    "        # order item by naif 1 array\n",
    "        recommendations = recommendations.sort_values(by='naif1', ascending=False)\n",
    "        #print (recommendations.iloc[:,2])\n",
    "\n",
    "        # calculate penalties \n",
    "        penalties = penalt(u_i=recommendations.iloc[:,2])\n",
    "        score_nf1 = score_nf1 + penalties\n",
    "\n",
    "\n",
    "        # --------------------- Perform on naif 2 --------------            \n",
    "        # order item by naif 2 array\n",
    "        recommendations = recommendations.sort_values(by='naif2', ascending=False)\n",
    "        #print (recommendations.iloc[:,2])\n",
    "\n",
    "        # calculate penalties \n",
    "        penalties = penalt(u_i=recommendations.iloc[:,2])                \n",
    "        score_nf2 = score_nf2 + penalties\n",
    "\n",
    "    # cases without any transaction\n",
    "    if (sum_mat == 0): \n",
    "        \n",
    "\n",
    "        divid = divid - 1\n",
    "\n",
    "# compute scores \n",
    "print ('scoreTOT')\n",
    "print (score/divid)\n",
    "print ('scoreNF1')\n",
    "print (score_nf1/divid)\n",
    "print ('scoreNF2')\n",
    "print (score_nf2/divid)\n",
    "\n",
    "score = score / divid\n",
    "score_nf2 = score_nf2 / divid\n",
    "score_nf1 = score_nf1 / divid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# représenter les données graphiquement \n",
    "d = {\"name\":['score_manuel','score_naif1','score_naif2','score_collaboratif'],\"value\":[score_manu,score_nf1,score_nf2,score]\n",
    "    }\n",
    "\n",
    "df_score = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the matplotlib figure\n",
    "f, (ax1) = plt.subplots(1, 1, figsize=(15, 9), sharex=True)\n",
    "\n",
    "# occurence en valeurs non manquantes\n",
    "y1 = df_score.iloc[:,1]\n",
    "g = sns.barplot(x=df_score.iloc[:,0], y=y1, palette='Blues_d', ax=ax1)\n",
    "ax1.axhline(0, color=\"k\", clip_on=False)\n",
    "\n",
    "# affiche uniquement un certain nombre de films en légende\n",
    "plt.locator_params(axis='x', nbins=65)\n",
    "#ax1.axes.get_xaxis().set_visible(False)\n",
    "ax1.set_xlabel(\"numéro d'utilisateur\",fontsize=15)\n",
    "ax1.set_ylabel(\"nombre de commandes\",fontsize=15)\n",
    "#plt.axis([0, 365, 0, 40])\n",
    "\n",
    "# Finalize the plot\n",
    "g = sns.despine(bottom=True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"nombre de commandes en fonction de l'utilisateur (pour les 1000 premiers utilisateurs)\",fontsize=20)\n",
    "#plt.setp(f.axes, yticks=[])\n",
    "plt.tight_layout(h_pad=2)\n",
    "plt.savefig('fig_commandes_utilisateurs.png', dpi=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
