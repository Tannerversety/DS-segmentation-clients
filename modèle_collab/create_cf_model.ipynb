{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    " \n",
    "data_cf = pd.read_csv('fc_matrix_red200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation de folds (modèle collaboratif)\n",
    "\n",
    "L'objectif de ce notebook est de créer plusieurs folds correspondant à différents trainset et testset.\n",
    "\n",
    "Ce procesus est indispensable aux choix des paramètres de nos futurs algorithmes de segmentation. En effet, dans le notebook \"analyse_cf_model\" on utilisera ces folds pour choisir notemment le nombre optimal de clusters (via un apprentisage supervisé. \n",
    "\n",
    "\n",
    "Nous choisissons une méthode non exaustive de création de test/trainset. Dans cette méthode nous fixons un nombre de folds, celui-ci nous donne une estimation de la quantité de transactions que l'on souhaite prendre en compte dans chaqu'un des jeux d'entrainements et de test. Autrement dit, on cherche à supprimer des transactions pour simuler des données inconnues. Ces transactions vont être supprimées de manière aléatoire à partir du jeux original pour aboutir au jeux d'entrainement. \n",
    "Sur 5 folds une transaction à donc une probabilité de 1/5 d'être supprimée. La colonne \"rating\" contient le nombre de transaction (user>item) identiques par utilisateur. Il faudra donc pondérer cette probabilité par la valeur contenue dans la colonne \"rating\". \n",
    "\n",
    "Par suite, on s'assurera, dans le notebook (analyse_cf_model) que les utilisateurs contenus dans le trainset et dans le testset sont identiques\n",
    "\n",
    "On notera que cette manière différe de celle utilisée pour générer des jeux d'entrainement et de test dans le modèle manuel. Cela s'explique par le fait que le modèle collaboratif est basé uniquement sur l'appréciation des produits par les utilisateurs. Les variables utilisateurs sont donc corrélées à la nature des items consommés. On ne tient pas ici compte explicitement des considérations temporelles par exemple ou transactionelles (qui nous contraignaient à sélectioner des commandes et non des transactions dans les jeux des test/entrainement) (voir notebook create_manual_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let us define some important functions \n",
    "\n",
    "\n",
    "def nonzeros(m, row):\n",
    "    for index in range(m.indptr[row], m.indptr[row+1]):\n",
    "        yield m.indices[index], m.data[index]\n",
    "        \n",
    "        \n",
    "\n",
    "def alternating_least_squares_cg(Cui, factors, regularization=0.01, iterations=15):\n",
    "    users, items = Cui.shape\n",
    "\n",
    "    # initialize factors randomly\n",
    "    X = np.random.rand(users, factors) * 0.01\n",
    "    Y = np.random.rand(items, factors) * 0.01\n",
    "\n",
    "    Cui, Ciu = Cui.tocsr(), Cui.T.tocsr()\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        least_squares_cg(Cui, X, Y, regularization)\n",
    "        least_squares_cg(Ciu, Y, X, regularization)\n",
    "\n",
    "    return (X, Y)\n",
    "\n",
    "\n",
    "def least_squares_cg(Cui, X, Y, regularization, cg_steps=3):\n",
    "    users, factors = X.shape\n",
    "    YtY = Y.T.dot(Y) + regularization * np.eye(factors)\n",
    "\n",
    "    for u in range(users):\n",
    "        # start from previous iteration\n",
    "        x = X[u]\n",
    "\n",
    "        # calculate residual r = (YtCuPu - (YtCuY.dot(Xu), without computing YtCuY\n",
    "        r = -YtY.dot(x)\n",
    "        for i, confidence in nonzeros(Cui, u):\n",
    "            r += (confidence - (confidence - 1) * Y[i].dot(x)) * Y[i]\n",
    "\n",
    "        p = r.copy()\n",
    "        rsold = r.dot(r)\n",
    "\n",
    "        for it in range(cg_steps):\n",
    "            # calculate Ap = YtCuYp - without actually calculating YtCuY\n",
    "            Ap = YtY.dot(p)\n",
    "            for i, confidence in nonzeros(Cui, u):\n",
    "                Ap += (confidence - 1) * Y[i].dot(p) * Y[i]\n",
    "\n",
    "            # standard CG update\n",
    "            alpha = rsold / p.dot(Ap)\n",
    "            x += alpha * p\n",
    "            r -= alpha * Ap\n",
    "            rsnew = r.dot(r)\n",
    "            p = r + (rsnew / rsold) * p\n",
    "            rsold = rsnew\n",
    "\n",
    "        X[u] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value_counts(col_name,df):       \n",
    "    \n",
    "    values_count = pd.DataFrame(df[col_name].dropna().value_counts())    \n",
    "    values_count.columns = ['count']\n",
    "    \n",
    "    # convert the index column into a regular column.\n",
    "    values_count[col_name] = [ str(i) for i in values_count.index ]\n",
    "    \n",
    "    # add a column with the percentage of each data point to the sum of all data points.\n",
    "    values_count['percent'] = values_count['count'].div(values_count['count'].sum()).multiply(100).round(2)\n",
    "    \n",
    "    # change the order of the columns.\n",
    "    values_count = values_count.reindex([col_name,'count','percent'],axis=1)\n",
    "    values_count.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    return (values_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traintest_split(df):\n",
    "    \"\"\"create the train and test set\"\"\" \n",
    "    \n",
    "    # looking for transaction cases\n",
    "    ind_rating = df.columns.get_loc('rating')\n",
    "    \n",
    "    df_trans = df[df.iloc[:,ind_rating]>0] \n",
    "    \n",
    "    # number of different transaction    \n",
    "    trans_nb = df_trans.shape[0]\n",
    "    \n",
    "    # define trainset and testset\n",
    "    \n",
    "    testset = df.copy()\n",
    "    trainset = df.copy()\n",
    "    \n",
    "    # get the rating indice \n",
    "    ind_rating = df.columns.get_loc('rating')\n",
    "    \n",
    "    # sort the dataset / put non zero rating in first\n",
    "    trainset = trainset.sort_values(by='rating', ascending=False)\n",
    "    \n",
    "    # for each non zero rating , for each transaction there is 1/5 probability to delete the transaction\n",
    "    trainset['iter'] = trainset.iloc[:,ind_rating] \n",
    "    \n",
    "    # get the rating indice \n",
    "    ind_iter = trainset.columns.get_loc('iter')    \n",
    "               \n",
    "    for i in range (int(np.amax(trainset.iloc[:,ind_rating]))):\n",
    "        \n",
    "        # apply the random filter max(trainset.iloc[:,ind_rating]) times \n",
    "        \n",
    "        shape = trainset.iloc[:,ind_rating][trainset.iloc[:,ind_iter]>0].shape[0]\n",
    "        trainset.iloc[:,ind_rating][trainset.iloc[:,ind_iter]>0] = trainset.iloc[:,ind_rating][\n",
    "                            trainset.iloc[:,ind_iter]>0] - (np.random.choice(2,(shape), p=[4/5,1/5])).transpose()\n",
    "        \n",
    "        # remove one to index iteration columns\n",
    "        \n",
    "        trainset.iloc[:,ind_iter] = trainset.iloc[:,ind_iter] - 1    \n",
    "     \n",
    "    # different transaction bewteen testset and trains\n",
    "    \n",
    "    testset.loc[:,'rating'] = df.loc[:,'rating'] - trainset.loc[:,'rating'] \n",
    "    trainset = shuffle(trainset)   \n",
    "    testset = shuffle(testset)\n",
    "        \n",
    "    return (trainset, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_full(df,folds,reg_param,alpha_val):\n",
    "    \"\"\"analyse the validity of our model and give the mean of the performance evaluate by cross validation\n",
    "       using our metric based on rank\"\"\"\n",
    "    \n",
    "    # switch datas to choose randomly user ID    \n",
    "    \n",
    "    df = shuffle(df)  \n",
    "    \n",
    "        \n",
    "    for k in range (folds): \n",
    "        \n",
    "        print (k)\n",
    "                \n",
    "        # ------------------------------------ SHARING DATASET (TRAIN/TEST) --------------------------------------\n",
    "        \n",
    "        # computer our train set and test set \n",
    "        print ('init traintest_split')\n",
    "        train_set, test_set = traintest_split(df=df) \n",
    "        print ('final traintest_split')\n",
    "        print (train_set.columns[:])\n",
    "        print (np.sum(train_set.loc[:,'rating']))\n",
    "        print (np.sum(test_set.loc[:,'rating']))\n",
    "        \n",
    "        \n",
    "        # ------------------------------------I -- COMPUTING ALGORITHM ON TRAIN SET --------------------------------------\n",
    "        \n",
    "        # take the data we need \n",
    "        data = train_set[['userID', 'itemID', 'rating']]\n",
    "\n",
    "        # Drop NaN columns\n",
    "        data = data.dropna()\n",
    "        data = data.copy()        \n",
    "\n",
    "        # Create a numeric user_id and artist_id column\n",
    "        data['userID'] = data['userID'].astype(\"category\")\n",
    "        data['itemID'] = data['itemID'].astype(\"category\")\n",
    "        data['user_id'] = data['userID'].cat.codes\n",
    "        data['item_id'] = data['itemID'].cat.codes    \n",
    "\n",
    "        # create two matricies, one for fitting the model (item-user) \n",
    "        # and one for recommendations (user-item)\n",
    "        sparse_item_user = sparse.csr_matrix((data['rating'].astype(float), (data['item_id'], data['user_id'])))\n",
    "        sparse_user_item = sparse.csr_matrix((data['rating'].astype(float), (data['user_id'], data['item_id'])))\n",
    "\n",
    "        # Calculate the confidence by multiplying it by our alpha value.\n",
    "        data_conf = (sparse_item_user * alpha_val).astype('double')\n",
    "\n",
    "        # Initialize the als model and fit it using the sparse item-user matrix\n",
    "        X, Y = alternating_least_squares_cg(Cui=data_conf ,factors=15, regularization=reg_param, iterations=20)\n",
    "\n",
    "        # Get the user and item vectors from our trained model\n",
    "        user_vecs_train = sparse.csr_matrix(X)\n",
    "        \n",
    "        \n",
    "         # ------------------------------------II -- COMPUTING ALGORITHM ON TEST SET --------------------------------------\n",
    "        \n",
    "        # take the data we need \n",
    "        data = test_set[['userID', 'itemID', 'rating']]\n",
    "\n",
    "        # Drop NaN columns\n",
    "        data = data.dropna()\n",
    "        data = data.copy()        \n",
    "\n",
    "        # Create a numeric user_id and artist_id column\n",
    "        data['userID'] = data['userID'].astype(\"category\")\n",
    "        data['itemID'] = data['itemID'].astype(\"category\")\n",
    "        data['user_id'] = data['userID'].cat.codes\n",
    "        data['item_id'] = data['itemID'].cat.codes    \n",
    "\n",
    "        # create two matricies, one for fitting the model (item-user) \n",
    "        # and one for recommendations (user-item)\n",
    "        sparse_item_user = sparse.csr_matrix((data['rating'].astype(float), (data['item_id'], data['user_id'])))\n",
    "        sparse_user_item = sparse.csr_matrix((data['rating'].astype(float), (data['user_id'], data['item_id'])))\n",
    "\n",
    "        # Calculate the confidence by multiplying it by our alpha value.\n",
    "        data_conf = (sparse_item_user * alpha_val).astype('double')\n",
    "\n",
    "        # Initialize the als model and fit it using the sparse item-user matrix\n",
    "        X, Y = alternating_least_squares_cg(Cui=data_conf ,factors=15, regularization=reg_param, iterations=20)\n",
    "\n",
    "        # Get the user and item vectors from our trained model\n",
    "        user_vecs_test = sparse.csr_matrix(X)\n",
    "        \n",
    "        # saving datas       \n",
    "        user_vecs_trtot = np.zeros((user_vecs_train.shape[0],user_vecs_train.shape[1]))\n",
    "        \n",
    "        for i in range (user_vecs_train.shape[0]):\n",
    "            for j in range (user_vecs_train.shape[1]):\n",
    "                user_vecs_trtot[i,j] = user_vecs_train[i,j]\n",
    "        d = {'1':user_vecs_trtot[:,0]}   \n",
    "        \n",
    "        for i in range (user_vecs_trtot.shape[1]-1):\n",
    "            d.update({'train{}'.format(i+1):user_vecs_trtot[:,i+1]})        \n",
    "        \n",
    "        dff = pd.DataFrame(data=d)\n",
    "        \n",
    "        dff.to_csv(\"user_vecs_train{}.csv\".format(k))\n",
    "        \n",
    "        user_vecs_tetot = np.zeros((user_vecs_test.shape[0],user_vecs_test.shape[1]))\n",
    "        \n",
    "        if (k == 0):\n",
    "            for i in range (user_vecs_test.shape[0]):\n",
    "                for j in range (user_vecs_test.shape[1]):\n",
    "                    user_vecs_tetot[i,j] = user_vecs_test[i,j]\n",
    "        \n",
    "        d = {'1':user_vecs_tetot[:,0]}   \n",
    "        \n",
    "        for i in range (user_vecs_tetot.shape[1]-1):\n",
    "            d.update({'train{}'.format(i+1):user_vecs_tetot[:,i+1]})\n",
    "        \n",
    "        dff = pd.DataFrame(data=d)\n",
    "        dff.to_csv(\"user_vecs_test{}.csv\".format(k))    \n",
    "                \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "init traintest_split\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tanne\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final traintest_split\n",
      "Index(['Unnamed: 0', 'try', 'itemID', 'userID', 'rating', 'iter'], dtype='object')\n",
      "33613.0\n",
      "8379.0\n",
      "1\n",
      "init traintest_split\n",
      "final traintest_split\n",
      "Index(['Unnamed: 0', 'try', 'itemID', 'userID', 'rating', 'iter'], dtype='object')\n",
      "33534.0\n",
      "8458.0\n",
      "2\n",
      "init traintest_split\n",
      "final traintest_split\n",
      "Index(['Unnamed: 0', 'try', 'itemID', 'userID', 'rating', 'iter'], dtype='object')\n",
      "33657.0\n",
      "8335.0\n",
      "3\n",
      "init traintest_split\n",
      "final traintest_split\n",
      "Index(['Unnamed: 0', 'try', 'itemID', 'userID', 'rating', 'iter'], dtype='object')\n",
      "33582.0\n",
      "8410.0\n",
      "4\n",
      "init traintest_split\n",
      "final traintest_split\n",
      "Index(['Unnamed: 0', 'try', 'itemID', 'userID', 'rating', 'iter'], dtype='object')\n",
      "33673.0\n",
      "8319.0\n"
     ]
    }
   ],
   "source": [
    "analyse_full(df=data_cf,folds=5,reg_param=0.1,alpha_val=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
